````markdown
# ğŸ¤– FAQBot | LoRA-Fine-Tuned FLAN-T5 Small

> ğŸš€ **Live Demo:** [Click here to try it on Hugging Face Spaces](https://huggingface.co/spaces/Piyush0001/faqbot-lora)

---

> ### ğŸ“Œ Project Overview
> FAQBot is a lightweight chatbot fine-tuned on a domain-specific FAQ dataset using **LoRA (Low-Rank Adaptation)** on `flan-t5-small`.  
> It's designed to be efficient, responsive, and deployable under cloud memory constraints â€” ideal for real-world inference at scale.

---

> ### âœ… Key Highlights
> - Fine-tuned using Hugging Face ğŸ¤— Transformers + PEFT + LoRA  
> - Model merged and **shrunk** for real-time inference  
> - FastAPI backend with async inference API  
> - Gradio UI frontend for interactive experience  
> - Fully containerized (Docker)  
> - Deployed on Hugging Face Spaces (100% free & fast!)

---

## ğŸ“– Whatâ€™s Inside?

```text
â”œâ”€â”€ data/                       # FAQ dataset
â”œâ”€â”€ output/                     # Merged, shrunk model
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                  # FastAPI or Gradio backend
â”‚   â”œâ”€â”€ finetune_llm.py         # LoRA training script
â”‚   â”œâ”€â”€ merge_lora.py           # Merge adapter weights
â”‚   â””â”€â”€ shrink_model.py         # Shrink model for deployment
â”œâ”€â”€ frontend/static/            # HTML/CSS/JS UI (if used)
â”œâ”€â”€ Dockerfile                  # Backend container
â”œâ”€â”€ requirements.txt            # Dependencies
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ render.yaml                 # Optional Render deployment config
````

---

## ğŸ“… Phase-wise Project Journey

> ### ğŸ§¹ Phase 1: Data Preparation
>
> * Cleaned and structured the FAQ dataset.
> * Converted to Q-A pairs in seq2seq format.
> * Split into train/validation using scripts & notebooks.

> ### ğŸ§  Phase 2: Fine-Tuning with LoRA
>
> * Used `flan-t5-small` base model.
> * Applied LoRA adapters via ğŸ¤— PEFT.
> * Efficient fine-tuning with reduced trainable parameters.
> * Tracked experiments using Weights & Biases.

> ### ğŸ§¬ Phase 3: Model Merging & Shrinking
>
> * Merged LoRA adapters into the base model.
> * Shrunk model size from \~950MB to \~300MB.
> * Saved and tested model for CPU inference.

> ### âš™ï¸ Phase 4: Backend API (FastAPI)
>
> * Developed async POST `/generate` endpoint.
> * Loaded tokenizer/model from Hugging Face.
> * Handled request/response formats.

> ### ğŸ’¬ Phase 5: Frontend UI (Gradio or Vanilla JS)
>
> * Created interactive UI to test chatbot.
> * Integrated with FastAPI or Gradio backend.
> * UI deployed via Hugging Face Spaces.

> ### ğŸ“¦ Phase 6: Deployment
>
> * Created Dockerfile for containerization.
> * Added `render.yaml` for optional Render deploy.
> * Final deployment done via Hugging Face Spaces (Gradio).
> * No GPU or heavy RAM required!

---

## ğŸ’¡ Why LoRA?

> âœ… LoRA fine-tuning drastically reduces compute requirements
> âœ… Retains model performance on small data
> âœ… Ideal for edge/cloud deployments

---

## ğŸš€ Run Locally

### ğŸ› ï¸ Setup

```bash
git clone https://github.com/Piyush0001/faqbot.git
cd faqbot
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### â–¶ï¸ Launch

```bash
uvicorn src.app:app --reload
```

Open `http://127.0.0.1:8000` in your browser
Or run `app.py` with Gradio interface for immediate UI testing.

---

## ğŸ“¦ Docker Support

```bash
# Build image
docker build -t faqbot .

# Run container
docker run -p 8000:8000 faqbot
```

---

## ğŸŒ Technologies Used

| Tool/Tech                 | Purpose                 |
| ------------------------- | ----------------------- |
| Python 3.10               | Core language           |
| Hugging Face Transformers | Model & tokenizer       |
| PEFT + LoRA               | Lightweight fine-tuning |
| FastAPI / Gradio          | API + Web interface     |
| Docker                    | Deployment and testing  |
| Git LFS                   | Managing large files    |
| Hugging Face Spaces       | Final deployment        |

---

## ğŸ§ª Challenges Overcome

> ğŸ§  Reduced model memory from >900MB to <350MB
> ğŸ”§ Debugged `render.com` crashes due to RAM overuse
> âš¡ Migrated from FastAPI to Gradio for lighter cloud deployment
> ğŸš› Used Git LFS for handling large model files cleanly

---

## ğŸŒ± Future Improvements

* [ ] Add conversational context memory (multi-turn chat)
* [ ] Improve UI with React or Next.js
* [ ] Integrate Pinecone or FAISS for RAG-based QA
* [ ] Add support for file uploads (PDF â†’ FAQ)

---

## ğŸ¤ Recruiter Notes

> âœ… Hands-on NLP project using LoRA
> âœ… Fully built, tested, and deployed end-to-end
> âœ… Covers fine-tuning, optimization, API dev, and cloud deploy
> âœ… Strong understanding of model serving and size trade-offs
> âœ… Open to collaboration or extensions for new domains!

---

## ğŸ“¬ Contact Me

**ğŸ‘¨â€ğŸ’» Piyush Kumar Mishra**
ğŸ“§ [mishra.piyush827@gmail.com](mailto:mishra.piyush827@gmail.com)
ğŸ”— [LinkedIn](https://linkedin.com/in/piyushkmishra)

---

Thank you for visiting the FAQBot repository.
Feel free to â­ the repo or [try the chatbot live](https://huggingface.co/spaces/Piyush0001/faqbot-lora)!

````